}
else
{
w_value <- door_values[1]
}
return(w_value)
}
simEVarW(100, c(1,2,3), 0.5, 1000)
simEVarW(10, c(1,2,10), 0.5, 1000)
simEVarW(10, c(1,2,3), 0.5, 1000)
sqrt(1:6)
sqrt(1:6) * rep(1/6, 6)
a <- c(1,2,3,4)
a * 4
m <- c(1,2,3)
k <- 10
s <- 5
tags <- function(m, k, s)
{
if (k <= 0)
{
if (s == 0)
{
return 1
tags <- function(m, k, s)
{
if (k <= 0)
{
if (s == 0)
{
return(1)
}
}
else tags(m, k - sample(m,1), s-1)
}
a <- vector()
for (i in 1:100)
{
a <- c(a, tags(m, k, s))
}
a
tags <- function(m, k, s)
{
if (k <= 0)
{
if (s == 0)
{
return(1)
}
else
{
return(0)
}
}
else tags(m, k - sample(m,1), s-1)
}
a <- vector()
for (i in 1:100)
{
a <- c(a, tags(m, k, s))
}
tags <- function(m, k, s)
{
if (k <= 0)
{
if (s == 0)
{
return(1)
}
else
{
return(0)
}
}
else tags(m, k - sample(m,1), s-1)
}
a <- vector()
for (i in 1:100)
{
a <- c(a, tags(m, k, s))
}
p <- mean(a)
print(p)
tags <- function(m, k, s) {
# base cases
if (m <= 0 | k <= 0 | s <= 0) {
return(0)
} else if (k <= m) {
return(1)
} else if (k > s * m) {
return(0)
}
# recursive cases
else {
p <- 0
for (i in 1:m) {
p <- p + tags(m, k - i, s - 1)
}
return(p/m)
}
}
tag(m,k,s)
tags(m,k,s)
tags <- function(m, k, s)
{
p <- 0
tag_help(m, k, s, p)
print(p/m)
}
tag_help <- function(m, k, s, p)
{
if (k <= 0)
{
if (s == 0)
{
return(1)
}
else
{
return(0)
}
}
for (i in 1:m)
{
p <- p + tag_help(m, k-i, s-1)
}
}
tags(m,k,s)
tags <- function(m, k, s)
{
p <- 0
for (i in 1:m)
{
p <- p + tag_help(m, k-i, s-1)
}
print(p/m)
}
tag_help <- function(m, k, s)
{
if (k <= 0)
{
if (s == 0)
{
return(1)
}
else
{
return(0)
}
}
}
tags(m,k,s)
tags <- function(m, k, s)
{
p <- 0
if (k <= 0)
{
if (s == 0)
{
return(1)
}
else
{
return(0)
}
}
for (i in 1:m)
{
p <- p + tags(m, k-i, s-1) * 1/m
}
print(p)
}
tags(m,k,s)
tags <- function(m, k, s)
{
if (k <= 0)
{
if (s == 0)
{
return(1)
}
else
{
return(0)
}
}
p <- 0
for (i in 1:m)
{
p <- p + tags(m, k-i, s-1) * 1/m
}
print(p)
}
tags(m,k,s)
p <- 0
tags(m,k,s)
m = 3
tags(m,k,s)
tags <- function(m, k, s)
{
if (k <= 0)
{
if (s == 0)
{
return(1)
}
else
{
return(0)
}
}
p <- 0
for (i in 1:m)
{
p <- p + tags(m, k-i, s-1) * 1/m
}
return(p)
}
tags(m,k,s)
tags(3,3,1)
tags(3,2,1)
rpois(1, 10)
rpois(1, 10)
rpois(1, 10)
rpois(1, 10)
rpois(1, 10)
rpois(1, 10)
rpois(1, 10)
rpois(1, 10)
rpois(1, 10)
rpois(1, 10)
rpois(1, 10)
rpois(1, 10)
rpois(1, 10)
rpois(1, 10)
rpois(1, 10)
tags(3,2,1)
rpois(1, 10)
rpois(1, 10)
rpois(1, 10)
rpois(1, 10)
rpois(1, 10)
rpois(1, 10)
rpois(1, 10)
?rpois()
matrix(rep(0,100),nrow=10)
onesixth <- 1/6
for(iin1:10){ #lookateachrow
for(iin1:10){ #lookateachrow
for(iin1:10){ #lookateachrow
for(iin1:10){
for(i in 1:10){
for (j in 1:6) {
k<-i+j
if (k > 10) k <- k - 10
p[i,k] <- onesixth
} }
p <- matrix(rep(0,100),nrow=10)
for(i in 1:10){
for (j in 1:6) {
k<-i+j
if (k > 10) k <- k - 10
p[i,k] <- onesixth
} }
View(p)
print(dbinom(3,10,0.15))
p = 5
n <- nrow(p)
imp <- diag(n) - t(p)
p = matrix(1, 5, 5)
n <- nrow(p)
n <- nrow(p)
n
imp <- diag(n) - t(p)
t(p)
diag(n)
p <- matrix(rep(0,9), nrow=3)
p[1,1] <- 0.5
p[1,2] <- 0.5
p[2,3] <- 0.5
p[2,1] <- 0.5
p[3,1] <- 1
p
findpi1(p)
0.5 ^ 3
c(rep(0, n-1), 1)
dunif(0.1,0,1)
dunif(0.1,0,1)
dunif(1,0,1)
dunif(1,0,10)
dunif(0.1,0,1)
dunif(0.2,0,1)
dunif(5,0,5)
dunif(5,0,100)
punif(1,0,100
)
punif(2,0,100)
qunif(1,0,100)
qunif(2,0,100)
qunif(3,0,100)
qunif(10,0,100)
qunif(100,0,100)
dunif(0.2,0,0.5)
qunif(0.2,0,100)
runif()
runif(1)
runif(1)
runif(1)
runif(1)
runif(1)
runif(1)
runif(1)
runif(1)
runif(1)
runif(1)
runif(1)
runif(1)
runif(1)
runif(1)
runif(1)
runif(1)
dunif(0.1, 0.1, 1)
dunif(0.1, 0, 1)
dunif(2, 0, 1)
import ggplot2
ggplot2
library(ggplot2)
# Generate x values ranging from -4 to 4
x <- seq(-4, 4, length.out = 100)
# Calculate PDF values for each x
pdf <- dnorm(x, mean = 0, sd = 1)
# Create a data frame for plotting
data <- data.frame(x = x, pdf = pdf)
# Plot the PDF
ggplot(data, aes(x, pdf)) +
geom_line() +
xlab("x") +
ylab("Probability Density") +
ggtitle("Standard Normal Distribution PDF")
q
q()
dpark <- function(p,d,i)
{
# subtract one from assumed state
ret <- dgeom(d + i - 1, p) + dgeom(d - i - 1, p)
}
ppark <- function(p,d,i)
{
# everything from d + i to d - i, so sutract everything from d - i - 1 onward. Also subtract 1.
ret <- pgeom(d + i - 1, p) - pgeom(d - i - 2, p)
}
qpark <- function(p,d,q)
{
}
rpark <- function(n, p, d)
{
# generate n parks and get abs value of those minus d
ret <- abs(rgeom(n, p) - d)
}
print(qpark(0.25, .578, 1))
print(rpark(10, 0.25, 12))
print(rpark(10, 0.01, 12))
# 8*8
MarkovMat <- matrix(0, nrow = 8, ncol = 8)
# Initialize as normal board possibility
for (each_line in 1:8)
{
for (each_cell in 1:8)
{
current_total <- each_line + each_cell
if (current_total > 8)
{
current_total <- current_total - 8
}
MarkovMat[each_line, current_total] = 1/6
}
}
View(MarkovMat)
# 8*8
MarkovMat <- matrix(0, nrow = 8, ncol = 8)
# Initialize as normal board possibility
for (each_line in 1:8)
{
for (each_cell in 1:6)
{
current_total <- each_line + each_cell
if (current_total > 8)
{
current_total <- current_total - 8
}
MarkovMat[each_line, current_total] = 1/6
}
}
# 8*8
MarkovMat <- matrix(0, nrow = 8, ncol = 8)
# Initialize as normal board possibility
for (each_line in 1:8)
{
for (each_cell in 1:6)
{
current_total <- each_line + each_cell
if (current_total > 8)
{
current_total <- current_total - 8
}
MarkovMat[each_line, current_total] = 1/6
}
}
# Sate 3 is a bonus, which means that no state can go into 3.
# So add 3's possibilty of each to other states
# Since originally those states who could go to state 3 only has 1/6 possiblity.
# When adding 3's possibilty to others, those possiblity 3 has should multiply by 1/6.
for (each_line in 1:8)
{
# Check if current state can go to state 3
if(MarkovMat[each_line, 3] > 0)
{
for (each_cell in 1:8)
{
MarkovMat[each_line, each_cell] <- MarkovMat[each_line, each_cell] + MarkovMat[3, each_cell] * 1/6
}
}
}
MarkovMat[3, ] <- 0
# Define the Markov matrix
markov_matrix <- matrix(c(0.3, 0.2, 0.5,
0.1, 0.7, 0.2,
0.4, 0.1, 0.5), nrow = 3, byrow = TRUE)
# Calculate the eigenvalues and eigenvectors
eigen_result <- eigen(t(markov_matrix))  # Transpose the matrix for column-wise eigenvectors
# Get the index of the eigenvalue equal to 1
index <- which(abs(eigen_result$values - 1) < 1e-8)
# Extract the corresponding eigenvector (stationary distribution)
stationary_distribution <- Re(eigen_result$vectors[, index])
# Normalize the stationary distribution to sum to 1
stationary_distribution <- stationary_distribution / sum(stationary_distribution)
# Display the long-run proportion of time in each state
proportion_of_time <- stationary_distribution
names(proportion_of_time) <- c("State 1", "State 2", "State 3")
print(proportion_of_time)
ls
setwd("/Users/lloydretro2/Desktop/ECS132/TermProject/Data/")
load("EDFfair/lawschoolbrief.RData",verbose=T)
View(lawschoolbrief)
# Replace directory with path to the Data folder
setwd("/Users/lloydretro2/Desktop/ECS132/TermProject/Data/")
load("EDFfair/lawschoolbrief.RData",verbose=T)
# Actual Plot
hist(lawschoolbrief$GPA, probability=TRUE)
plot(density(lawschoolbrief$GPA))
# Save to the local variable
gpa <- lawschoolbrief$GPA
# Average
A <- mean(gpa) / 5
# Variance
S2 <- var(gpa) / 25
# alpha
alpha <- A * ((A * (1 - A) / S2) - 1)
# beta
beta <- alpha / A - alpha
curve(dbeta(x/5, alpha, beta), 0, 5)
N <- length(gpa)
logbar <- mean(log(gpa/5))
logbarp <- mean(log(1 - gpa/5))
nLL <- function(alpha, beta)
{
# loglik <- (alpha - 1) * logbar + (beta - 1) * logbarp - N * log(beta(alpha, beta))
loglik <- log(prod(dbeta(gpa/5, alpha, beta)))
return(-loglik)
}
z <- stats4::mle(minuslog=nLL, start = list(alpha = .1, beta = .1))
set> setwd("/Users/lloydretro2/Desktop/ECS132/TermProject/Data/")
> setwd("/Users/lloydretro2/Desktop/ECS132/TermProject/Data/")
setwd("/Users/lloydretro2/Desktop/ECS132/TermProject/Data/")
ls
load("fairml/communities.and.crime.rda",verbose=T)
View(communities.and.crime)
load("fairml/national.longitudinal.survey.rda",verbose=T)
View(national.longitudinal.survey)
hist(national.longitudinal.survey&weight)
hist(national.longitudinal.survey$weight)
hist(national.longitudinal.survey$height)
hist(national.longitudinal.survey$age)
plot(density(national.longitudinal.survey$weight))
plot(density(national.longitudinal.survey$age))
plot(density(national.longitudinal.survey$height))
plot(density(national.longitudinal.survey$famsize))
plot(density(national.longitudinal.survey$afqt89))
plot(density(national.longitudinal.survey$jobsnum90))
plot(density(national.longitudinal.survey$age))
plot(density(national.longitudinal.survey$illegalact))
plot(density(national.longitudinal.survey$charged))
plot(density(national.longitudinal.survey$jobsnum90))
View(communities.and.crime)
plot(density(communities.and.crime$racePctWhite))
plot(density(communities.and.crime$racePctAsian))
load("qeML/weatherTS.RData",verbose=T)
View(weatherTS)
plot(density(weatherTS$PRECTOT))
hist(weatherTS$PRECTOT)
View(lawschoolbrief)
View(weatherTS)
plot(density(weatherTS$RH2M))
plot(density(weatherTS$T2M))
plot(density(weatherTS$RH2M))
View(communities.and.crime)
View(data)
View(lawschoolbrief)
View(weatherTS)
View(national.longitudinal.survey)
View(national.longitudinal.survey)
plot(density(national.longitudinal.survey$weight))
